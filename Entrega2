# Medicao-Trabalho-Final

# Plano de Experimento – Scoping e Planejamento

## **1. Identificação básica**

## **1.1 Título do experimento**
Avaliação comparativa de técnicas de testes automatizados (Selenium vs. Cypress) em um aplicativo web.

## **1.2 ID / código**
EXP-TST-AUTO-001

## **1.3 Versão do Documento e Histórico de Revisão**
v1.0 (23/11/2025) — Versão inicial.
v1.1 (24/11/2025) — Adição de tabelas GQM e métricas detalhadas.

## **1.4 Datas (criação, última atualização)**
- Criação: 23/11/2025
- Última atualização: 24/11/2025

## **1.5 Autores (nome, área, contato)**
Gabriel Oliveira – Engenharia de Software – gabrielfoliveira@hotmail.com

## **1.6 Responsável principal (PI / dono do experimento)**
Gabriel Oliveira

## **1.7 Projeto / produto / iniciativa relacionada**

* **Projeto de pesquisa aplicado em Engenharia de Software**, com foco na melhoria de processos de qualidade.
* Relacionado a **ferramentas de automação de testes** utilizadas para apoiar o desenvolvimento e aumentar a produtividade das equipes.
* Contribui para a tomada de decisão sobre **seleção de ferramentas** em ambientes reais de desenvolvimento de software.

## **2. Contexto e Problema**

## **2.1 Descrição do problema / oportunidade**

Equipes de desenvolvimento frequentemente enfrentam dificuldade para definir qual ferramenta utilizar na automação de testes funcionais. As ferramentas mais populares, **Selenium** e **Cypress**, apresentam diferentes vantagens, mas não há consenso sobre qual oferece **melhor eficácia**, **tempo de execução menor** e **mais estabilidade**.
O experimento busca responder essa lacuna comparando as duas ferramentas em condições controladas.

---

## **2.2 Contexto organizacional e técnico**

O experimento será conduzido em um **aplicativo web simples**, representativo de cenários comuns do mercado.
O ambiente inclui:

* pipeline de desenvolvimento padrão;
* execução dos testes em máquina local;
* ferramentas **open-source**;
* foco em **testes funcionais automatizados**.
  O propósito é simular a realidade de uma equipe que precisa escolher a ferramenta mais adequada sem grandes investimentos.

---

## **2.3 Trabalhos e evidências prévias (internos e externos)**

A base do experimento inclui:

* documentação oficial do **Selenium** e do **Cypress**;
* estudos acadêmicos sobre automação de testes e qualidade de software;
* benchmarks existentes na literatura que analisam desempenho, manutenção e estabilidade;
* relatos de uso das ferramentas por desenvolvedores em empresas e comunidades técnicas.
  Essas evidências mostram a relevância prática e acadêmica da comparação.

---

## **2.4 Referencial teórico e empírico essencial**

Os principais conceitos que embasam o experimento incluem:

* **Testes funcionais automatizados** e suas etapas;
* **Métricas de qualidade de software** (tempo de execução, taxa de falhas, manutenibilidade);
* Modelo **GQM (Goal–Question–Metric)** para estruturar objetivos e métricas;
* Comparações empíricas de ferramentas de automação realizadas em artigos científicos;
* Características técnicas das ferramentas, como arquitetura, API, linguagem de scripting e suporte à comunidade.

## 3. Objetivos e Questões (GQM)

## 3.1 Objetivo Geral
Analisar as ferramentas Selenium WebDriver e Cypress com o propósito de caracterizar sua eficácia, eficiência e usabilidade sob a perspectiva do desenvolvedor/testador no contexto de projetos de automação de testes web em aplicações de pequeno e médio porte.

## 3.2 Objetivos Específicos
- **O1:** Comparar a eficiência no desenvolvimento de testes
- **O2:** Avaliar o desempenho na execução dos testes
- **O3:** Mensurar a confiabilidade e estabilidade dos testes
- **O4:** Analisar a usabilidade e facilidade de manutenção

## 3.3 Tabela GQM (Goal-Question-Metric)

| Objetivo | Pergunta | Métricas |
|----------|----------|----------|
| **O1:** Eficiência no desenvolvimento | Q1.1: Qual ferramenta permite desenvolvimento mais rápido? | M1, M2 |
| | Q1.2: Qual ferramenta requer menos esforço cognitivo? | M3, M4 |
| | Q1.3: Qual ferramenta possui melhor integração com ferramentas de desenvolvimento? | M5, M6 |
| **O2:** Desempenho na execução | Q2.1: Qual ferramenta executa testes mais rapidamente? | M7, M8 |
| | Q2.2: Qual ferramenta consome menos recursos computacionais? | M9, M10 |
| | Q2.3: Qual ferramenta escala melhor com aumento de testes? | M7, M11 |
| **O3:** Confiabilidade e estabilidade | Q3.1: Qual ferramenta produz testes mais estáveis? | M12, M13 |
| | Q3.2: Qual ferramenta possui melhor tratamento de elementos assíncronos? | M14, M15 |
| | Q3.3: Qual ferramenta oferece melhor feedback em falhas? | M16, M17 |
| **O4:** Usabilidade e manutenção | Q4.1: Qual ferramenta é mais fácil de aprender? | M18, M19 |
| | Q4.2: Qual ferramenta facilita a depuração? | M20, M21 |
| | Q4.3: Qual ferramenta possui documentação mais eficaz? | M22, M23 |

## 3.4 Tabela de Métricas Detalhadas

| Métrica | Descrição | Unidade |
|---------|------------|---------|
| **M1** - Tempo de desenvolvimento | Tempo total para implementar conjunto padrão de testes | Horas |
| **M2** - Velocidade de codificação | Número de linhas de código por hora | LOC/hora |
| **M3** - Complexidade cognitiva | Pontuação em escala de complexidade percebida | Pontos (1-5) |
| **M4** - Curva de aprendizado | Tempo para implementar primeiro teste funcional | Minutos |
| **M5** - Integração com IDE | Facilidade de configuração e uso com VS Code | Pontos (1-5) |
| **M6** - Qualidade de autocompletar | Eficácia do autocompletar e sugestões de código | Pontos (1-5) |
| **M7** - Tempo de execução total | Tempo para executar suite completa de testes | Segundos |
| **M8** - Tempo médio por teste | Tempo médio de execução por teste individual | Segundos/teste |
| **M9** - Uso de CPU | Percentual de uso da CPU durante execução | Percentual (%) |
| **M10** - Uso de memória RAM | Consumo de memória durante execução | Megabytes (MB) |
| **M11** - Overhead de execução | Tempo adicional por teste ao aumentar suite | Segundos/teste |
| **M12** - Taxa de sucesso | Percentual de testes que passam consistentemente | Percentual (%) |
| **M13** - Flaky tests | Número de testes com comportamento inconsistente | Quantidade |
| **M14** - Estabilidade em waits | Eficácia no tratamento de elementos dinâmicos | Pontos (1-5) |
| **M15** - Timeouts necessários | Número de waits explícitos necessários | Quantidade |
| **M16** - Clareza de mensagens | Qualidade das mensagens de erro | Pontos (1-5) |
| **M17** - Screenshots automáticos | Qualidade e utilidade de screenshots automáticos | Pontos (1-5) |
| **M18** - Facilidade inicial | Facilidade para configuração inicial | Pontos (1-5) |
| **M19** - Intuitividade da API | Facilidade para entender e usar a API | Pontos (1-5) |
| **M20** - Ferramentas de debug | Qualidade das ferramentas de depuração | Pontos (1-5) |
| **M21** - Logs detalhados | Utilidade dos logs gerados | Pontos (1-5) |
| **M22** - Qualidade documentação | Clareza e completude da documentação | Pontos (1-5) |
| **M23** - Exemplos práticos | Qualidade e quantidade de exemplos | Pontos (1-5) |

## 4. Escopo e Contexto do Experimento

## 4.1 Escopo Funcional/de Processo
**Incluído:**
- Testes das funcionalidades: Login, CRUD de produtos, carrinho de compras, checkout
- Desenvolvimento de testes idênticos em ambas ferramentas
- Coleta de todas as 23 métricas definidas
- Análise qualitativa e quantitativa

**Excluído:**
- Testes de performance (carga/stress)
- Testes de segurança
- Testes cross-browser
- Integração com CI/CD complexa

## 4.2 Contexto do Estudo
Experimento acadêmico conduzido por único pesquisador com experiência intermediária em desenvolvimento web e conhecimentos básicos em ambas ferramentas, simulando cenário de adoção inicial em pequenas equipes.

## 4.3 Premissas
- Aplicação web demo permanece estável durante experimento
- Ambiente de desenvolvimento consistentemente configurado
- Documentações oficiais são fontes confiáveis

## 4.4 Restrições
- Tempo máximo: 4 semanas
- Recursos: 1 computador pessoal
- Orçamento: zero (ferramentas open-source)

## 4.5 Limitações Previstas
- Resultados limitados a aplicações web similares à demo
- Percepções subjetivas de único pesquisador
- Ambiente controlado pode não refletir ambientes corporativos

## 5. Stakeholders e Impacto Esperado

## 5.1 Stakeholders Principais
- Pesquisador/autor do TCC
- Orientador acadêmico
- Comunidade de desenvolvedores/testadores
- Equipes de desenvolvimento de software

## 5.2 Interesses e Expectativas
- Evidências concretas para seleção de ferramentas
- Entendimento de trade-offs entre Selenium e Cypress
- Diretrizes para adoção em diferentes cenários

## 5.3 Impactos Potenciais
- Influência na decisão de ferramentas em projetos futuros
- Melhor entendimento dos pontos fortes e fracos de cada ferramenta
- Redução de tempo e custo em processos de seleção tecnológica

## 6. Riscos de Alto Nível e Critérios de Sucesso

## 6.1 Riscos de Alto Nível
- **Técnico:** Instabilidade na aplicação demo
- **Tempo:** Não conclusão dentro do prazo do TCC
- **Validade:** Resultados não generalizáveis

## 6.2 Critérios de Sucesso Globais
- Coleta completa de pelo menos 20 das 23 métricas
- Análise estatisticamente significativa das métricas quantitativas
- Documentação clara dos procedimentos e resultados

## 6.3 Critérios de Parada Antecipada
- Impossibilidade de configurar ambas ferramentas
- Problemas críticos na aplicação demo não resolvíveis
- Restrições de tempo que impossibilitem coleta mínima de dados


